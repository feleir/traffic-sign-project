{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a3c668",
   "metadata": {},
   "source": [
    "# Sign processing\n",
    "\n",
    "This notebook starts by fetching observations from the WFS defined layer, the main idea is to convert the sps scripts to python code, which we could then execute directly via github and/or process without having to have SPSS installed.\n",
    "The notebook requires `pandas`, as per the instructions in the requirements file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6556be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owslib.wfs import WebFeatureService\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6d714",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Configuration variables are defined here, this is only temporary since this code will all be converted to scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d23b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://opendata.apps.mow.vlaanderen.be/opendata-geoserver/awv/wfs?version=2.0.0\" #&service=wfs&request=GetCapabilities\"\n",
    "wfs = WebFeatureService(url=url, version=\"2.0.0\", timeout=3600)\n",
    "vb_type_name = \"awv:Verkeersborden.Vlaanderen_Borden\"\n",
    "\n",
    "# Configuration\n",
    "# Output file where we will store the WFS results\n",
    "feature_output_file = \"output.csv\"\n",
    "# Previous processed data, used to filter out previous data\n",
    "previous_processed_date = \"31/07/2022\"\n",
    "# Previous traffic signs\n",
    "traffic_signs_info = \"../find-interesting-signs/road_signs_cleaned.csv\"\n",
    "# Traffic sign processing output file\n",
    "processing_output_file = \"maproulette.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982ba69",
   "metadata": {},
   "source": [
    "## Fetch number of features\n",
    "Fetch all the features for the required layer from the WFS service, we use this later on to query for them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_features_by_type(feature_type):\n",
    "    response = wfs.getfeature(typename=feature_type, outputFormat=\"json\", maxfeatures=1)\n",
    "    r = response.read()\n",
    "    d = r.decode('UTF-8')\n",
    "    j = json.loads(d)\n",
    "    return j['totalFeatures'] \n",
    "\n",
    "total_features = get_total_features_by_type(vb_type_name)\n",
    "print(\"{}: #features = {}\".format(datetime.now(), total_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5be87",
   "metadata": {},
   "source": [
    "## Obtain and store the signs\n",
    "Fetch data from WFS, rmeove line breaks and store into the defined csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_linebreaks(data):\n",
    "    replace1 = data.replace(b'\\n',b' ')\n",
    "    replace2 = replace1.replace(b'\\r ',b'\\r\\n')\n",
    "    return replace2\n",
    "\n",
    "def get_and_store_features(file_name, feature_type, max_features):\n",
    "    response = wfs.getfeature(typename=feature_type, maxfeatures=max_features, outputFormat=\"csv\", startindex=0)\n",
    "    cleaned_response = remove_linebreaks(response.read())\n",
    "    decoded_response = cleaned_response.decode('UTF-8')\n",
    "\n",
    "    with open(file=file_name, encoding='UTF-8', mode='w', newline='') as csvfile:\n",
    "        csvfile.write(decoded_response)\n",
    "        \n",
    "print(\"{}: Starting fetching data from WFS service\".format(datetime.now()))\n",
    "get_and_store_features(feature_output_file, vb_type_name, total_features)\n",
    "print(\"{}: WFS data stored in {}\".format(datetime.now(), feature_output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0aeb82",
   "metadata": {},
   "source": [
    "## Process data\n",
    "\n",
    "Load the signs data in `panda` dataframes, this data is filtered by the `previous_processed_date` and joined with the signs metadata by `bordcode`.\n",
    "\n",
    "**Note:** All this code is dataset specific, ideally this should be abstracted away, including column definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b21bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv(feature_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb8f3a",
   "metadata": {},
   "source": [
    "### Date filtering\n",
    "\n",
    "Filter the dataframe for all signs with date greater than the `previous_processed_date` configuration value. This is done by: 1) converting the `datum_plaatsing` to date in the `date` column, and 2) filtering the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The file containes {len(feature_df)} features before filtering by date.\")\n",
    "feature_df['date'] = pd.to_datetime(feature_df['datum_plaatsing'], errors = 'coerce', infer_datetime_format=True)\n",
    "filter_mask = feature_df['date'].notna() & (dataframe[\"date\"] > previous_processed_date)\n",
    "filtered_df = feature_df[filter_mask]\n",
    "print(f\"The file contains {len(filtered_df)} features after filtering by date greater than {previous_processed_date}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19140ce4",
   "metadata": {},
   "source": [
    "### Data parsing anc vonersion\n",
    "\n",
    "Some small conversion on the `bordcode` field, as per the SPS code. This code also create the identifier removing the string from the `FID` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bordcode processing, remove Z from it and add (zone) description.\n",
    "filtered_df['bordcode'] = filtered_df.apply(lambda row: (f\"{row['bordcode'][1:]} (zone)\" if row['bordcode'].startswith('Z') else row['bordcode']).replace(\"/\", \"\"), axis=1)\n",
    "# Replace strings from FID\n",
    "filtered_df['id'] = filtered_df['FID'].str.replace('Verkeersborden.Vlaanderen_Borden.','')\n",
    "filtered_df.drop(columns=['FID'])\n",
    "# This will need require some cleaning on the parameters as well. Probably better to do it before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef18969",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_metadata = pd.read_csv(traffic_signs_info, sep=\";\", encoding = \"ISO-8859-1\")\n",
    "sign_metadata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5bff0",
   "metadata": {},
   "source": [
    "### Join and grouping\n",
    "\n",
    "Merge the sign metadata with the current dataset based on the `bordcode` field. Then group by `id_aanzicht` to identified clustered signs. After that we get the required values and store them based on `processing_output_file` configuration value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708150c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join both datasets by the bordcode\n",
    "joined_df = filtered_df.join(sign_metadata.set_index(\"bordcode\"), on='bordcode')\n",
    "# Remove NaN parameters and name\n",
    "joined_df[['parameters', 'name']] = joined_df[['parameters','name']].fillna('')\n",
    "joined_df.dtypes\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766fad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = joined_df.groupby('id_aanzicht', as_index=False).agg({\n",
    "     'opinion': 'max', \n",
    "     'bordcode': ' | '.join,\n",
    "     'locatie_x': 'max',\n",
    "     'locatie_y': 'max',\n",
    "     'parameters': lambda x : '|'.join(y for y in x if y != ''),\n",
    "     'name': lambda x : '|'.join(y for y in x if y != ''),\n",
    "     'datum_plaatsing': 'max',\n",
    "     'id': 'max'})\n",
    "grouped_df = grouped_df[grouped_df['opinion'] > 0]\n",
    "print(f\"Found {len(grouped_df)} signs after grouping by id_aanzicht\")\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grouped_df.rename(columns = {\n",
    "    \"bordcode\": \"traffic_sign_code\", \n",
    "    \"parameters\": \"extra_text\",\n",
    "    \"datum_plaatsing\": \"date_installed\",\n",
    "    \"name\": \"traffic_sign_description\"\n",
    "})[['id', 'traffic_sign_code', 'extra_text', 'traffic_sign_description', 'date_installed', 'locatie_x', 'locatie_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(processing_output_file, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1df1f1",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "The panda dataframe should be stored in geojson format already, including the comversion to ESPG:4326.\n",
    "\n",
    "```\n",
    "open in QGIS as Lambert 72, EPSG 31370), save as GeoJSON in ESPG:4326\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16600024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
