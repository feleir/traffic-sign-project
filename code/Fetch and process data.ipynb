{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a3c668",
   "metadata": {},
   "source": [
    "# Sign processing\n",
    "\n",
    "This notebook starts by fetching observations from the WFS defined layer, the main idea is to convert the sps scripts to python code, which we could then execute directly via github and/or process without having to have SPSS installed.\n",
    "The notebook requires `pandas`, as per the instructions in the requirements file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6556be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owslib.wfs import WebFeatureService\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import folium\n",
    "from pandas_geojson import to_geojson, write_geojson\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6d714",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Configuration variables are defined here, this is only temporary since this code will all be converted to scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d23b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfs_url = 'https://opendata.apps.mow.vlaanderen.be/opendata-geoserver/awv/wfs'\n",
    "vb_type_name = \"awv:Verkeersborden.Vlaanderen_Borden\"\n",
    "\n",
    "# Configuration\n",
    "# Output file where we will store the WFS results\n",
    "feature_output_file = \"../python_output/feature_output.csv\"\n",
    "# Output where the csv file will be stored\n",
    "signs_csv_output_file = \"../python_output/signs_output.csv\"\n",
    "# Output where the geojson file will be stored\n",
    "geojson_output_file = \"../python_output/geojson_output.json\"\n",
    "# Previous processed data, used to filter out previous data\n",
    "previous_processed_date = \"2022-07-31\"\n",
    "# Previous traffic signs\n",
    "traffic_signs_info = \"../find-interesting-signs/road_signs_cleaned.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982ba69",
   "metadata": {},
   "source": [
    "## Fetch number of features\n",
    "Fetch all the features for the required layer from the WFS service, we use this later on to query for them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_features_by_type(feature_type):\n",
    "    response = requests.get(wfs_url, params={\n",
    "     'service': 'WFS',\n",
    "     'version': '2.0.0',\n",
    "     'request': 'GetFeature',\n",
    "     'typename': feature_type,\n",
    "     'outputFormat': 'json',\n",
    "     'count': 1\n",
    "    })\n",
    "    j = json.loads(response.content)\n",
    "    return j['totalFeatures'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5be87",
   "metadata": {},
   "source": [
    "## Obtain and store the signs\n",
    "Fetch data from WFS, remove line breaks and store into the defined csv file.\n",
    "https://opendata.apps.mow.vlaanderen.be/opendata-geoserver/awv/wfs?version=2.0.0&request=GetCapabilities defines the capabilities, `GetFeature` by default has a count of 10M features. There are actual problems fetching the whole dataset from the WFS service as it fails sometimes, the url to fetch the feature is in https://opendata.apps.mow.vlaanderen.be/opendata-geoserver/awv/wfs?version=2.0.0&service=WFS&version=2.0.0&request=GetFeature&typeName=awv%3AVerkeersborden.Vlaanderen_Borden&outputFormat=csv the code below retries until the saved csv has pulled the required features from the WFS service.\n",
    "\n",
    "The exception below indicates that the underlying datastore has been updated while in the process of pulling data from the WFS server.\n",
    "\n",
    "```\n",
    "<ows:Exception exceptionCode=\"NoApplicableCode\">\n",
    "<ows:ExceptionText>java.lang.RuntimeException: org.postgresql.util.PSQLException: ERROR: canceling statement due to conflict with recovery\n",
    "  Detail: User query might have needed to see row versions that must be removed.\n",
    "org.postgresql.util.PSQLException: ERROR: canceling statement due to conflict with recovery\n",
    "  Detail: User query might have needed to see row versions that must be removed.\n",
    "ERROR: canceling statement due to conflict with recovery\n",
    "  Detail: User query might have needed to see row versions that must be removed.</ows:ExceptionText>\n",
    "</ows:Exception>\n",
    "</ows:ExceptionReport>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e912e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_and_store_features(file_name, feature_type, max_features):\n",
    "    response = requests.get(wfs_url, params={\n",
    "     'service': 'WFS',\n",
    "     'version': '2.0.0',\n",
    "     'request': 'GetFeature',\n",
    "     'typename': feature_type,\n",
    "     'outputFormat': 'csv',\n",
    "     'count': max_features\n",
    "    })\n",
    "    with open(file=file_name, encoding='UTF-8', mode='w', newline='') as csvfile:\n",
    "        csvfile.write(response.content.decode('UTF-8'))\n",
    "        \n",
    "pending_processing = True\n",
    "try_count = 0\n",
    "while pending_processing:\n",
    "    try_count += 1\n",
    "    print(\"{}: {} WFS get feature.\".format(datetime.now(), try_count))\n",
    "    total_features = get_total_features_by_type(vb_type_name)\n",
    "    print(\"{}: #features = {}\".format(datetime.now(), total_features))\n",
    "    print(\"{}: Starting fetching data from WFS service, total features {}\".format(datetime.now(), total_features))\n",
    "    get_and_store_features(feature_output_file, vb_type_name, total_features)\n",
    "    print(\"{}: WFS data stored in {}\".format(datetime.now(), feature_output_file))\n",
    "    stored_features_df = pd.read_csv(feature_output_file)\n",
    "    total_stored_features = len(stored_features_df.index)\n",
    "    print(\"{}: Stored {} features in the csv.\".format(datetime.now(), total_stored_features))\n",
    "    pending_processing = total_stored_features < total_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0aeb82",
   "metadata": {},
   "source": [
    "## Process data\n",
    "\n",
    "Load the signs data in `panda` dataframes, this data is filtered by the `previous_processed_date` and joined with the signs metadata by `bordcode`.\n",
    "\n",
    "**Note:** All this code is dataset specific, ideally this should be abstracted away, including column definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b21bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv(feature_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb8f3a",
   "metadata": {},
   "source": [
    "### Date filtering\n",
    "\n",
    "Filter the dataframe for all signs with date greater than the `previous_processed_date` configuration value. This is done by: 1) converting the `datum_plaatsing` to date in the `date` column, and 2) filtering the dataframe.\n",
    "\n",
    "**note** Filter new dates and previous date.\n",
    "**note** The filtering is being applied on US format rather than european."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b82bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['date'] = pd.to_datetime(feature_df['datum_plaatsing'], errors = 'coerce', format='%d/%m/%Y')\n",
    "filter_mask = feature_df['date'].notna() \\\n",
    "    & (feature_df[\"date\"] > previous_processed_date) \\\n",
    "    & (feature_df['date'] < (pd.Timestamp.today() + pd.Timedelta('1D')))\n",
    "filtered_df = feature_df[filter_mask]\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The file containes {len(feature_df)} features before filtering by date.\")\n",
    "feature_df['date'] = pd.to_datetime(feature_df['datum_plaatsing'], errors = 'coerce', format='%d/%m/%Y')\n",
    "filter_mask = feature_df['date'].notna() \\\n",
    "    & (feature_df[\"date\"] > previous_processed_date) \\\n",
    "    & (feature_df['date'] < (pd.Timestamp.today() + pd.Timedelta('1D')))\n",
    "filtered_df = feature_df[filter_mask]\n",
    "print(f\"The file contains {len(filtered_df)} features after filtering by date greater than {previous_processed_date}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19140ce4",
   "metadata": {},
   "source": [
    "### Data parsing and conversion\n",
    "\n",
    "Some small conversion on the `bordcode` field, as per the SPS code. This code also create the identifier removing the string from the `FID` value. Latitude and longitude are converted from [EPSG:31370](https://epsg.io/31370) to [EPSG:4326](https://epsg.io/4326) aka WGS84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "transformer = Transformer.from_crs(\"epsg:31370\", \"epsg:4326\", always_xy=True)\n",
    "\n",
    "def convertCoords(row):\n",
    "    # Transform columns based on locatie_x (longitude) and locatie_y (latitude).\n",
    "    longitude ,latitude = transformer.transform(row['locatie_x'],row['locatie_y'])\n",
    "    return pd.Series({'longitude': longitude,'latitude': latitude})\n",
    "\n",
    "# convert coordinates\n",
    "filtered_df[['longitude','latitude']] = filtered_df.apply(convertCoords,axis=1)\n",
    "# Bordcode processing, remove Z from it and add (zone) description.\n",
    "filtered_df['bordcode'] = filtered_df.apply(lambda row: (f\"{row['bordcode'][1:]} (zone)\" if row['bordcode'].startswith('Z') else row['bordcode']).replace(\"/\", \"\"), axis=1)\n",
    "# Replace strings from FID\n",
    "filtered_df['id'] = filtered_df['FID'].str.replace('Verkeersborden.Vlaanderen_Borden.','')\n",
    "filtered_df.drop(columns=['FID'])\n",
    "# This will need require some cleaning on the parameters as well. Probably better to do it before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef18969",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_metadata = pd.read_csv(traffic_signs_info, sep=\";\", encoding = \"ISO-8859-1\")\n",
    "sign_metadata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5bff0",
   "metadata": {},
   "source": [
    "### Join and grouping\n",
    "\n",
    "Merge the sign metadata with the current dataset based on the `bordcode` field. Then group by `id_aanzicht` to identified clustered signs. After that we get the required values and store them based on `processing_output_file` configuration value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708150c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join both datasets by the bordcode\n",
    "joined_df = filtered_df.join(sign_metadata.set_index(\"bordcode\"), on='bordcode')\n",
    "# Remove NaN parameters and name\n",
    "joined_df[['parameters', 'name']] = joined_df[['parameters','name']].fillna('')\n",
    "joined_df.dtypes\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766fad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = joined_df.groupby('id_aanzicht', as_index=False).agg({\n",
    "     'opinion': 'max', \n",
    "     'bordcode': ' | '.join,\n",
    "     'latitude': 'max',\n",
    "     'longitude': 'max',\n",
    "     'parameters': lambda x : '|'.join(y for y in x if y != ''),\n",
    "     'name': lambda x : '|'.join(y for y in x if y != ''),\n",
    "     'datum_plaatsing': 'max',\n",
    "     'id': 'max'})\n",
    "grouped_df = grouped_df[grouped_df['opinion'] > 0]\n",
    "print(f\"Found {len(grouped_df)} signs after grouping by id_aanzicht\")\n",
    "display(grouped_df)\n",
    "grouped_df.to_csv(signs_csv_output_file, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grouped_df.rename(columns = {\n",
    "    \"bordcode\": \"traffic_sign_code\", \n",
    "    \"parameters\": \"extra_text\",\n",
    "    \"datum_plaatsing\": \"date_installed\",\n",
    "    \"name\": \"traffic_sign_description\"\n",
    "})[['id', 'traffic_sign_code', 'extra_text', 'traffic_sign_description', 'date_installed', 'longitude', 'latitude']]\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520bd61",
   "metadata": {},
   "source": [
    "# Store results\n",
    "Store the processing results in geojson format using `pandas_geojson`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4079a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geo_json = to_geojson(df=result, lat='latitude', lon='longitude',\n",
    "                 properties=['id','traffic_sign_code','extra_text','traffic_sign_description', 'date_installed' ])\n",
    "write_geojson(geo_json, filename=geojson_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001faafa",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "Simple visualization of the geojson results in folium, no custom popup for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folium_map = folium.Map(\n",
    "    location=[50.8476, 4.3572],\n",
    "    zoom_start=8,\n",
    ")\n",
    "folium.GeoJson(data=geo_json).add_to(folium_map)\n",
    "folium_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16600024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
